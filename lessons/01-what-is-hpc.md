What is a supercomputer and why do we care?
-------------------------------------------
>*Learning Objectives*
>*    Student will understand that a supercomputer is made up of many computers, called nodes, that work together over a high speed network.
>*    Student will be able to demonstrate an understanding of what kind of problems would be helped by a supercomputer and what kind of problems would not be helped by a supercomputer.
>*    Students will be able to define the terms `parallel` and `serial`, and their implications for super computing.

What is a Supercomputer?
------------------------
Supercomputing has a long rich history in academic research. Click [here](https://www.youtube.com/watch?v=6TRfy70DqD8 "Ferranti Atlas: Britain's first supercomputer") to see a video of what supercomputing looked like at its inception. A common misconception of supercomputing is that supercomputers are giant, futuristic machines computing massive calculations that cannot be done on a personal computer. While supercomputers do allow their users to push beyond the capabilities of a personal computer, they are not actually a single machine. In fact, supercomputers are simply many “normal” computers that have been connected by a high speed network that allows them to work together on a problem that would be too big for any single computer to take on by itself. Most supercomputers simply look like the server racks that are found in a data center. One supercomputer can have anywhere from two (not very useful) to hundreds of single computers networked together. Each of the single computers are referred to as a node.
![Oklahoma State University Cowboy Supercomputer](/img/cowboy.jpg "Cowboy Supercomputer")

Parallel Teamwork
-----------------
Having many computers working together on a research problem can be especially helpful if the project requires large amounts of computing or repetitive tasks. Supercomputers tend to be particularly useful for problems that lend themselves to parallelization. A problem that can be parallelized is a problem that can be broken into many smaller sub-problems that do not depend on each other in order to be solved. For example, consider the chemist who is working on a simulation that requires six calculations per molecule with each calculation taking at least twelve hours to complete. Our poor chemist will need seventy-two hours to finish simulating one molecule. Unfortunately, it gets worse. Our chemist wants to be able to look at systems of at least fifty molecules! That means he will need at least 150 days of uninterrupted computation on his computer. Clearly our chemist needs a faster solution. The chemist can let each node on the supercomputer compute a single particle and his job will now only take seventy-two hours instead of 150 days (Special thanks to Shanaka Paranahewage and Bryan Dunsmore of Oklahoma State University for this example).
Our chemist's problem is an example of a problem that can be computed in parallel. In this case, the supercomputer was a good choice for the chemist since each node could do computations for a single molecule. In general, supercomputers are a good tool for solving parallel problems.

Serial Complications
--------------------
In the chemist example, we have assumed that calculations for one molecule do not depend on the calculations of another molecule. Real life is not always so simple. This is why the nodes in a supercomputer need to communicate with each other over a network. This dependence can be so extreme that each computation depends explicitly on a previous computation. *(Need a more realistic example here) Consider the meteorologist who wants to create a climate model forecast for the next several years. The meteorologist is interested in the temperature at a single location, one year from today. Forecasting the temperature in a given day is dependent upon temperatures from the previous day. If the computation for a single day requires two hours on her personal computer, she will need 730 hours, or a month of uninterrupted computation in order to compute the 365 different day computations necessary to forecast the temperature one year from today. It would be convenient if she could use a supercomputer to speed up the process by giving each node the computations for a single day. Then her job would only take two hours. Unfortunately, since each day depends on the previous day's results for its computations, each node will wait in line for the previous node to finish and it will still take a month for the job to finish even though she is using the supercomputer.* Such a job is said to be a serial job. Supercomputers do not give a speed up advantage for serial jobs.

Breaking Up a Problem
---------------------
In reality, meteorologists are actually heavy supercomputer users, but they do not break up their problems in this manner. Learning how to break up a problem for computation is an important skill when using a supercomputer. Before even using a supercomputer it important to consider how a problem can be broken into smaller pieces, and to identify which parts are serial and which parts are parallel. This allows researchers to identify whether or not a supercomputer will provide worthwhile speed-up for a particular problem. Often a problem will have a mixture of both parallel and serial aspects. The staff at the High Performance Computing Center are an excellent resource and can help researchers as they identify how to implement their particular problem on a supercomputer. They can connect researchers with both local and national resources. [Don't hesitate to contact them](https://hpcc.okstate.edu/content/osu-hpc-contact-information "OSU HPCC Contact Info").

Pete's Tweets
-----------------------
Pete is a geography researcher who has never used a supercomputer before. Pete is working with geographical twitter data. There's a lot of data on twitter and the sheer amount of data he needs to work with means his personal computer is not going to provide him enough processing power, so he is going to need to use Cowboy.
